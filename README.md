# AdaTAD

[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/end-to-end-temporal-action-detection-with-1b/temporal-action-localization-on-thumos14)](https://paperswithcode.com/sota/temporal-action-localization-on-thumos14?p=end-to-end-temporal-action-detection-with-1b)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/end-to-end-temporal-action-detection-with-1b/temporal-action-localization-on-activitynet)](https://paperswithcode.com/sota/temporal-action-localization-on-activitynet?p=end-to-end-temporal-action-detection-with-1b)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/end-to-end-temporal-action-detection-with-1b/temporal-action-localization-on-epic-kitchens)](https://paperswithcode.com/sota/temporal-action-localization-on-epic-kitchens?p=end-to-end-temporal-action-detection-with-1b)


This repo holds of CVPR2024 paper: ["AdaTAD: End-to-End Temporal Action Detection with 1B Parameters Across 1000 Frames"](https://arxiv.org/abs/2311.17241)

- The official pytorch implementation and pretrained checkpoints are released at [**OpenTAD**](https://github.com/sming256/OpenTAD/tree/main/configs/adatad).
- We achieve an average mAP of 42.90% on ActivityNet and 77.07% on THUMOS14. Please have a check!

## Citation

```latex
@InProceedings{Liu_2024_CVPR,
    author    = {Liu, Shuming and Zhang, Chen-Lin and Zhao, Chen and Ghanem, Bernard},
    title     = {End-to-End Temporal Action Detection with 1B Parameters Across 1000 Frames},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2024},
    pages     = {18591-18601}
}
```
